"""
–ú–æ–¥—É–ª—å –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (fuzzy matching) –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è Q-–∫–æ–¥–æ–≤, –ø–æ–∑—ã–≤–Ω—ã—Ö –∏ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
"""
from difflib import SequenceMatcher
from functools import lru_cache
import re

# –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
try:
    from .levenshtein_optimized import levenshtein_fast
    USE_OPTIMIZED_LEVENSHTEIN = True
except ImportError:
    USE_OPTIMIZED_LEVENSHTEIN = False


@lru_cache(maxsize=1024)
def levenshtein_distance(s1, s2):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏
    (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å—Ç–∞–≤–æ–∫, —É–¥–∞–ª–µ–Ω–∏–π, –∑–∞–º–µ–Ω)
    –ö–µ—à–∏—Ä—É–µ—Ç—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é numba-–≤–µ—Ä—Å–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    if USE_OPTIMIZED_LEVENSHTEIN:
        return levenshtein_fast(s1, s2)
    
    # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            # j+1 –≤–º–µ—Å—Ç–æ j —Ç–∞–∫ –∫–∞–∫ previous_row –∏ current_row –Ω–∞ 1 –¥–ª–∏–Ω–Ω–µ–µ s2
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row

    return previous_row[-1]
@lru_cache(maxsize=512)
def similarity_ratio(s1, s2):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0 - 1.0)
    1.0 = –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, 0.0 = –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–Ω—ã–µ
    –ö–µ—à–∏—Ä—É–µ—Ç—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0 - 1.0)
    1.0 = –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, 0.0 = –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–Ω—ã–µ
    """
    return SequenceMatcher(None, s1, s2).ratio()


def fuzzy_match_q_code(word, q_codes_dict, max_distance=1):
    """
    –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ Q-–∫–æ–¥–∞ —Å –¥–æ–ø—É—Å–∫–æ–º –æ—à–∏–±–æ–∫
    
    Args:
        word: —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        q_codes_dict: —Å–ª–æ–≤–∞—Ä—å Q-–∫–æ–¥–æ–≤ {–∫–æ–¥: –æ–ø–∏—Å–∞–Ω–∏–µ}
        max_distance: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (–æ–±—ã—á–Ω–æ 1-2)
    
    Returns:
        tuple (matched_code, meaning, confidence) –∏–ª–∏ None
    """
    word = word.upper()
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if word in q_codes_dict:
        return (word, q_codes_dict[word], 1.0)
    
    # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
    best_match = None
    best_distance = float('inf')
    
    for code in q_codes_dict:
        distance = levenshtein_distance(word, code)
        
        if distance <= max_distance and distance < best_distance:
            best_distance = distance
            best_match = code
    
    if best_match:
        # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (confidence)
        # 0 –æ—à–∏–±–æ–∫ = 1.0, 1 –æ—à–∏–±–∫–∞ = 0.8, 2 –æ—à–∏–±–∫–∏ = 0.6
        confidence = 1.0 - (best_distance * 0.2)
        return (best_match, q_codes_dict[best_match], confidence)
    
    return None


def fuzzy_match_callsign(word, known_callsigns=None, max_distance=1):
    """
    –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ–∑—ã–≤–Ω–æ–≥–æ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫
    
    Args:
        word: —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        known_callsigns: —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ–∑—ã–≤–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        max_distance: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    
    Returns:
        tuple (matched_callsign, confidence) –∏–ª–∏ (word, confidence_as_callsign)
    """
    word = word.upper()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–æ–∑—ã–≤–Ω–æ–≥–æ (—É–ª—É—á—à–µ–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
    if not _is_likely_callsign(word):
        return None
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ–∑—ã–≤–Ω—ã—Ö, –∏—â–µ–º –≤ –Ω—ë–º
    if known_callsigns:
        best_match = None
        best_distance = float('inf')
        
        for callsign in known_callsigns:
            distance = levenshtein_distance(word, callsign)
            
            if distance <= max_distance and distance < best_distance:
                best_distance = distance
                best_match = callsign
        
        if best_match:
            confidence = 1.0 - (best_distance * 0.2)
            return (best_match, confidence)
    
    # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ—Ç, –Ω–æ —Å–ª–æ–≤–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–æ–∑—ã–≤–Ω–æ–π
    confidence = _calculate_callsign_confidence(word)
    if confidence > 0.5:
        return (word, confidence)
    
    return None


def _is_likely_callsign(word):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ—Ö–æ–∂–µ –ª–∏ —Å–ª–æ–≤–æ –Ω–∞ –ø–æ–∑—ã–≤–Ω–æ–π"""
    if len(word) < 3 or len(word) > 10:
        return False
    
    # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏ –±—É–∫–≤—ã, –∏ —Ü–∏—Ñ—Ä—ã
    has_letter = any(c.isalpha() for c in word)
    has_digit = any(c.isdigit() for c in word)
    
    if not (has_letter and has_digit):
        return False
    
    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    russian_prefixes = ['R', 'U', 'RA', 'RU', 'UA', 'RK', 'RN', 'RZ', 'RW', 'RV']
    for prefix in russian_prefixes:
        if word.startswith(prefix):
            return True
    
    # –î—Ä—É–≥–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    common_prefixes = ['K', 'W', 'N', 'G', 'DL', 'F', 'I', 'JA', 'VE', 'VK', 'ZL', 'OH']
    for prefix in common_prefixes:
        if word.startswith(prefix):
            return True
    
    # –û–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç: –±—É–∫–≤—ã + —Ü–∏—Ñ—Ä–∞ + –±—É–∫–≤—ã
    pattern = r'^[A-Z]{1,3}\d[A-Z]{1,4}$'
    if re.match(pattern, word):
        return True
    
    return False


def _calculate_callsign_confidence(word):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —á—Ç–æ —Å–ª–æ–≤–æ - –ø–æ–∑—ã–≤–Ω–æ–π (0.0-1.0)"""
    confidence = 0.0
    
    # –î–ª–∏–Ω–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    if 4 <= len(word) <= 7:
        confidence += 0.3
    elif 3 <= len(word) <= 8:
        confidence += 0.2
    
    # –ï—Å—Ç—å —Ü–∏—Ñ—Ä–∞
    if any(c.isdigit() for c in word):
        confidence += 0.2
    
    # –†–æ—Å—Å–∏–π—Å–∫–∏–π –ø—Ä–µ—Ñ–∏–∫—Å
    if word.startswith(('R', 'U', 'RA', 'RU', 'UA')):
        confidence += 0.3
    
    # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –±—É–∫–≤/—Ü–∏—Ñ—Ä
    letters = sum(c.isalpha() for c in word)
    digits = sum(c.isdigit() for c in word)
    if 1 <= digits <= 2 and letters >= 3:
        confidence += 0.2
    
    return min(confidence, 1.0)


def fuzzy_match_prosign(word, prosigns_dict, max_distance=1):
    """
    –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞
    
    Args:
        word: —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        prosigns_dict: —Å–ª–æ–≤–∞—Ä—å prosigns {–∫–æ–¥: –æ–ø–∏—Å–∞–Ω–∏–µ}
        max_distance: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    
    Returns:
        tuple (matched_prosign, meaning, confidence) –∏–ª–∏ None
    """
    word = word.upper()
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if word in prosigns_dict:
        return (word, prosigns_dict[word], 1.0)
    
    # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
    best_match = None
    best_distance = float('inf')
    
    for prosign in prosigns_dict:
        distance = levenshtein_distance(word, prosign)
        
        if distance <= max_distance and distance < best_distance:
            best_distance = distance
            best_match = prosign
    
    if best_match:
        confidence = 1.0 - (best_distance * 0.25)  # –°—Ç—Ä–æ–∂–µ –¥–ª—è prosigns
        return (best_match, prosigns_dict[best_match], confidence)
    
    return None


def contextual_code_enhancement(words, position, detected_codes):
    """
    –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤
    
    Args:
        words: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        position: –ø–æ–∑–∏—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞
        detected_codes: —É–∂–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–¥—ã
    
    Returns:
        dict —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
    """
    context = {
        'is_message_end': False,
        'is_message_start': False,
        'likely_callsign_context': False,
        'likely_number': False
    }
    
    current_word = words[position] if position < len(words) else ''
    prev_word = words[position - 1] if position > 0 else ''
    next_word = words[position + 1] if position + 1 < len(words) else ''
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω–µ—Ü —Å–æ–æ–±—â–µ–Ω–∏—è
    # "73" –æ–±—ã—á–Ω–æ –≤ –∫–æ–Ω—Ü–µ, —á–∞—Å—Ç–æ –ø–æ—Å–ª–µ "TU" (Thank You)
    if current_word in ['73', '88']:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±–ª–∏–∑–∫–æ –ª–∏ –∫ –∫–æ–Ω—Ü—É —Ç–µ–∫—Å—Ç–∞
        if position > len(words) * 0.7:
            context['is_message_end'] = True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—á–∞–ª–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    # "CQ" –æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ, "DE" –ø–µ—Ä–µ–¥ –ø–æ–∑—ã–≤–Ω—ã–º
    if current_word == 'CQ' and position < len(words) * 0.3:
        context['is_message_start'] = True
    
    if prev_word == 'DE' or current_word == 'DE':
        context['likely_callsign_context'] = True
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    # "NR" (–Ω–æ–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è), "CHECK" (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
    if prev_word in ['NR', 'CHECK', 'RST']:
        context['likely_number'] = True
    
    return context


def smart_code_detection(text, q_codes, prosigns, z_codes=None, max_errors=1):
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–¥–æ–≤ —Å –Ω–µ—á–µ—Ç–∫–∏–º –ø–æ–∏—Å–∫–æ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
    
    Args:
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        q_codes: —Å–ª–æ–≤–∞—Ä—å Q-–∫–æ–¥–æ–≤
        prosigns: —Å–ª–æ–≤–∞—Ä—å –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        z_codes: —Å–ª–æ–≤–∞—Ä—å Z-–∫–æ–¥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        max_errors: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ (1-2)
    
    Returns:
        dict —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –∫–æ–¥–∞–º–∏ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
    """
    words = text.upper().split()
    
    results = {
        'q_codes': [],
        'prosigns': [],
        'callsigns': [],
        'z_codes': [],
        'confidence_stats': {
            'high_confidence': 0,  # >= 0.9
            'medium_confidence': 0,  # 0.7-0.9
            'low_confidence': 0,  # < 0.7
        }
    }
    
    for i, word in enumerate(words):
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        context = contextual_code_enhancement(words, i, results)
        
        # –ü–æ–∏—Å–∫ Q-–∫–æ–¥–æ–≤
        q_match = fuzzy_match_q_code(word, q_codes, max_distance=max_errors)
        if q_match:
            code, meaning, confidence = q_match
            results['q_codes'].append({
                'word': word,
                'matched_code': code,
                'meaning': meaning,
                'confidence': confidence,
                'exact_match': word == code
            })
            _update_confidence_stats(results['confidence_stats'], confidence)
        
        # –ü–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        prosign_match = fuzzy_match_prosign(word, prosigns, max_distance=max_errors)
        if prosign_match:
            code, meaning, confidence = prosign_match
            results['prosigns'].append({
                'word': word,
                'matched_code': code,
                'meaning': meaning,
                'confidence': confidence,
                'exact_match': word == code
            })
            _update_confidence_stats(results['confidence_stats'], confidence)
        
        # –ü–æ–∏—Å–∫ Z-–∫–æ–¥–æ–≤
        if z_codes:
            z_match = fuzzy_match_q_code(word, z_codes, max_distance=max_errors)
            if z_match:
                code, meaning, confidence = z_match
                results['z_codes'].append({
                    'word': word,
                    'matched_code': code,
                    'meaning': meaning,
                    'confidence': confidence,
                    'exact_match': word == code
                })
                _update_confidence_stats(results['confidence_stats'], confidence)
        
        # –ü–æ–∏—Å–∫ –ø–æ–∑—ã–≤–Ω—ã—Ö
        if not q_match and not prosign_match:  # –ù–µ –∏—â–µ–º –ø–æ–∑—ã–≤–Ω—ã–µ –≤ —Å–ª–æ–≤–∞—Ö, —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–∞–∫ –∫–æ–¥—ã
            callsign_match = fuzzy_match_callsign(word)
            if callsign_match:
                matched, confidence = callsign_match
                # –ü–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π
                if context['likely_callsign_context']:
                    confidence = min(confidence + 0.1, 1.0)
                
                results['callsigns'].append({
                    'word': word,
                    'matched_callsign': matched,
                    'confidence': confidence,
                    'exact_match': word == matched,
                    'context': context
                })
                _update_confidence_stats(results['confidence_stats'], confidence)
    
    return results


def _update_confidence_stats(stats, confidence):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
    if confidence >= 0.9:
        stats['high_confidence'] += 1
    elif confidence >= 0.7:
        stats['medium_confidence'] += 1
    else:
        stats['low_confidence'] += 1


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
    print("üîç –ú–æ–¥—É–ª—å Fuzzy Matching –¥–ª—è Morse Decoder")
    print("\n–ü—Ä–∏–º–µ—Ä—ã:")
    
    # –¢–µ—Å—Ç Q-–∫–æ–¥–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    q_codes_test = {
        'QRZ': '–ö—Ç–æ –º–µ–Ω—è –≤—ã–∑—ã–≤–∞–µ—Ç?',
        'QTH': '–ú–æ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ',
        'QSL': '–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é –ø—Ä–∏–µ–º'
    }
    
    test_words = ['QRX', 'QTB', 'QSL', 'QR2']  # QRX –±–ª–∏–∑–∫–æ –∫ QRZ, QTB –±–ª–∏–∑–∫–æ –∫ QTH
    
    for word in test_words:
        match = fuzzy_match_q_code(word, q_codes_test, max_distance=1)
        if match:
            code, meaning, conf = match
            print(f"  '{word}' ‚Üí '{code}' ({meaning}) [—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf:.2f}]")
        else:
            print(f"  '{word}' ‚Üí –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # –¢–µ—Å—Ç –ø–æ–∑—ã–≤–Ω—ã—Ö
    print("\n–¢–µ—Å—Ç –ø–æ–∑—ã–≤–Ω—ã—Ö:")
    test_callsigns = ['RA3XYZ', 'UA1ABC', 'R5DX', 'K3LR', 'W1AW', 'HELLO', 'TEST123']
    
    for call in test_callsigns:
        match = fuzzy_match_callsign(call)
        if match:
            matched, conf = match
            print(f"  '{call}' ‚Üí –ø–æ–∑—ã–≤–Ω–æ–π [—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf:.2f}]")
        else:
            print(f"  '{call}' ‚Üí –Ω–µ –ø–æ–∑—ã–≤–Ω–æ–π")
